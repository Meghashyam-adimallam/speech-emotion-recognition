
# ğŸ™ï¸ Speech Emotion Recognition with PyTorch + Streamlit

A Deep Learning app that detects **emotions from voice recordings** using MFCC audio features and a BiLSTM model. Built with PyTorch for training and Streamlit for a real-time, interactive UI.

---

## ğŸš€ Features

ğŸ”Š Upload `.wav` speech audio files  
ğŸ§  Extract MFCC features using Librosa  
ğŸ“š Trained BiLSTM (Bidirectional LSTM) for emotion classification  
ğŸ“ˆ Evaluate using confusion matrix, precision, recall, F1  
ğŸ’¬ Streamlit app for real-time voice emotion prediction  
ğŸ§¼ Clean UI with upload, play, and prediction  
ğŸ” Dataset not included due to size (RAVDESS must be downloaded manually)

---

## ğŸ“ Project Structure

```

speech-emotion-recognition/
â”œâ”€â”€ app.py                 # Streamlit web app
â”œâ”€â”€ train.py               # Model training
â”œâ”€â”€ evaluate.py            # Test evaluation and confusion matrix
â”œâ”€â”€ emotion\_bilstm.pth     # Trained model weights
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py           # BiLSTM architecture
â”‚   â”œâ”€â”€ dataset.py         # PyTorch dataset and loader
â”‚   â”œâ”€â”€ preprocessing.py   # MFCC extraction
â”‚   â””â”€â”€ **init**.py
â””â”€â”€ README.md

````

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/Meghashyam-adimallam/speech-emotion-recognition.git
cd speech-emotion-recognition
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install torch torchaudio librosa matplotlib seaborn scikit-learn streamlit
```

### 3. Download the dataset (RAVDESS)

Download only the **speech audio subset** from:
ğŸ“¥ [https://zenodo.org/record/1188976](https://zenodo.org/record/1188976)

Place the extracted folder like this:

```
data/raw_audio/Audio_Speech_Actors_01-24/
```

### 4. Train the model (optional)

```bash
python train.py
```

### 5. Evaluate on test set

```bash
python evaluate.py
```

### 6. Launch the web app

```bash
streamlit run app.py
```

---

## ğŸ’¡ How It Works

1. Uploads `.wav` file and loads audio
2. Extracts MFCCs (Mel-frequency cepstral coefficients)
3. Feeds the features into a trained BiLSTM
4. Predicts one of 8 emotion classes
5. Displays results in a user-friendly web UI

---

## ğŸ§  Emotion Classes

```python
['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']
```

---

## ğŸ“ Example Use Cases

ğŸ§ Build emotion-aware chatbots
ğŸ™ï¸ Analyze call center customer sentiment
ğŸ§  Use in mental health voice analysis
ğŸ“ˆ Real-time emotion detection from voice commands

---

## ğŸ“Š Model Performance

* Validation Accuracy: \~48â€“50%
* Strong predictions for: `angry`, `surprised`, `calm`
* Evaluated using accuracy, confusion matrix, and F1-score

---

[MIT License](LICENSE)

---

## ğŸ™Œ Acknowledgements

* RAVDESS dataset by Ryerson University
* Librosa for audio feature extraction
* PyTorch for deep learning training
* Streamlit for the UI
* Made with â¤ï¸ by Meghashyam Adimallam

````

---
